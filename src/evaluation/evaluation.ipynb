{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ultralytics import YOLO \n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "weights_path = \"../../models/\" \n",
    "results_path = \"../../results\"\n",
    "train_dir_path = \"/opt/homebrew/datasets/VisDrone/VisDrone2019-DET-train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning of YOLOv8 on VisDrone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(weights_path + \"yolov8n.pt\")\n",
    "results = model.train(data=\"VisDrone.yaml\", epochs=19, imgsz=640, device = \"mps\", save=True, amp=False, batch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(weights_path + \"yolov8n_finetuned_visdrone_19_epochs.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative evaluation of models on VisDrone val set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will evaluate the performance of the models on the VisDrone validation set, to assess the impact of the finetuning on the performance of YOLOv8n on the object recognition task on UAV images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_visdrone_yolov8n = YOLO(weights_path + \"yolov8n_finetuned_visdrone_19_epochs.pt\")\n",
    "regular_visdrone_yolov8n = YOLO(weights_path + \"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_finetuned_yolov8n = finetuned_visdrone_yolov8n.val(data=\"VisDrone.yaml\", imgsz=640, device=\"cpu\", save_json=True, batch=8) \n",
    "metrics_regular_yolov8n = regular_visdrone_yolov8n.val(data=\"VisDrone.yaml\", imgsz=640, device=\"cpu\", save_json=True, batch=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results of the models on VisDrone test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will be dedicated to the visualization of the results of the models on the test set. The results will be visualized in the form of bounding boxes on the images. We will evaluate different models including : the yolov8n model pretrained on the COCO dataset, the yolov8n model pretrained on COCO and finetuned on the VisDrone dataset. We hope to see that the latter will perform better than the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch \n",
    "\n",
    "data_path_visdrone = \"../../data/still_frames/VisDrone2019DETtest/\"\n",
    "\n",
    "finetuned_visdrone_yolov8n = YOLO(weights_path + \"yolov8n_finetuned_visdrone_19_epochs.pt\")\n",
    "regular_visdrone_yolov8n = YOLO(weights_path + \"yolov8n.pt\")\n",
    "\n",
    "np.random.seed(42)  # setting seed for reproducibility\n",
    "\n",
    "test_images = os.listdir(data_path_visdrone + \"/images\")\n",
    "test_images = [os.path.join(data_path_visdrone, \"images\", img) for img in test_images]\n",
    "test_images = np.random.choice(test_images, 80, replace=False)\n",
    "\n",
    "def run_inference(model, test_images, device=\"cpu\", save_results=None):\n",
    "    results = []\n",
    "\n",
    "    if device == \"mps\" and not torch.backends.mps.is_available():\n",
    "        print(\"MPS device is not available. Falling back to CPU.\")\n",
    "        device = \"cpu\"\n",
    "    elif device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA device is not available. Falling back to CPU.\")\n",
    "        device = \"cpu\"\n",
    "\n",
    "    print(f\"Running inference on device: {device}\")\n",
    "\n",
    "\n",
    "    for img_path in test_images:\n",
    "        try:\n",
    "            result = model(img_path, device=device, verbose=False, imgsz=640)\n",
    "            if save_results is not None:\n",
    "                if not os.path.exists(save_results):\n",
    "                    os.makedirs(save_results)\n",
    "                result[0].save(filename=os.path.join(save_results, f\"{os.path.basename(img_path)}\"))\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {str(e)}\")\n",
    "\n",
    "    print(f\"Successfully processed {len(results)} images\")\n",
    "    return results\n",
    "\n",
    "\n",
    "results_finetuned_yolov8n = run_inference(finetuned_visdrone_yolov8n, test_images, device=\"cpu\", save_results=os.path.join(results_path, \"finetuned_yolov8n\"))\n",
    "results_regular_yolov8n = run_inference(regular_visdrone_yolov8n, test_images, device=\"cpu\", save_results=os.path.join(results_path, \"regular_yolov8n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_frames_finetuned_yolo8n = [r[0].plot() for r in results_finetuned_yolov8n]\n",
    "annotated_frames_regular_yolo8n = [r[0].plot() for r in results_regular_yolov8n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_yolo_comparison(annotated_frames_finetuned, annotated_frames_regular, num_samples=10):\n",
    "    fig, axs = plt.subplots(num_samples, 2, figsize=(20, num_samples * 5))\n",
    "    fig.suptitle(\"Finetuned YOLOv8n vs Regular YOLOv8n on VisDrone dataset\", fontsize=16)\n",
    "\n",
    "    # Randomly selecting a subset of images to display\n",
    "    available_indices = list(range(len(annotated_frames_finetuned)))\n",
    "    selected_indices = np.random.choice(available_indices, num_samples, replace=False)\n",
    "\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        axs[i, 0].imshow(annotated_frames_regular[idx])\n",
    "        axs[i, 0].set_title(f\"Regular YOLOv8n - Image {idx}\")\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        axs[i, 1].imshow(annotated_frames_finetuned[idx])\n",
    "        axs[i, 1].set_title(f\"Fine-tuned YOLOv8n - Image {idx}\")\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.969)  \n",
    "    plt.show()\n",
    "\n",
    "plot_yolo_comparison(annotated_frames_finetuned_yolo8n, annotated_frames_regular_yolo8n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will evaluate the finetuned and regular yolov8n models on the visdrone test set. We will use the following metrics to evaluate how frugal the models are:\n",
    "- Inference time per frame, in milliseconds\n",
    "- Speed when running on CPU with onnx conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_visdrone_yolov8n.export(format=\"onnx\", simplify=True, dynamic=True, opset=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_visdrone_yolov8n = YOLO(weights_path + \"/yolov8n_finetuned_visdrone_19_epochs.pt\")\n",
    "finetuned_visdrone_onnx_yolov8n = YOLO(weights_path + \"/yolov8n_finetuned_visdrone_19_epochs.onnx\")\n",
    "\n",
    "results_finetuned_yolov8n = run_inference(finetuned_visdrone_yolov8n, test_images, device=\"cpu\", save_results=os.path.join(results_path, \"finetuned_yolov8n_onnx\"))\n",
    "results_finetuned_onnx_yolov8n = run_inference(finetuned_visdrone_onnx_yolov8n, test_images, device=\"cpu\", save_results=os.path.join(results_path, \"finetuned_onnx_yolov8n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "speed_data_finetuned_yolov8n = [r[0].speed for r in results_finetuned_yolov8n]\n",
    "speed_data_finetuned_yolov8n_onnx = [r[0].speed for r in results_finetuned_onnx_yolov8n]\n",
    "\n",
    "mean_processing_time_finetuned_yolov8n = np.mean([r[\"preprocess\"] for r in speed_data_finetuned_yolov8n])\n",
    "mean_inference_time_finetuned_yolov8n = np.mean([r[\"inference\"] for r in speed_data_finetuned_yolov8n])\n",
    "mean_postprocessing_time_finetuned_yolov8n = np.mean([r[\"postprocess\"] for r in speed_data_finetuned_yolov8n])\n",
    "\n",
    "mean_processing_time_finetuned_yolov8n_onnx = np.mean([r[\"preprocess\"] for r in speed_data_finetuned_yolov8n_onnx])\n",
    "mean_inference_time_finetuned_yolov8n_onnx = np.mean([r[\"inference\"] for r in speed_data_finetuned_yolov8n_onnx])\n",
    "mean_postprocessing_time_finetuned_yolov8n_onnx = np.mean([r[\"postprocess\"] for r in speed_data_finetuned_yolov8n_onnx])\n",
    "\n",
    "speed_data_benchmark = pd.DataFrame({\n",
    "    \"Model\": [\"YOLOv8n\", \"YOLOv8n ONNX\"], #\"YOLOv8s\", \"YOLOv8s ONNX\"],\n",
    "    \"Mean Preprocessing Time (ms)\": [mean_processing_time_finetuned_yolov8n, mean_processing_time_finetuned_yolov8n_onnx], #mean_processing_time_finetuned_yolov8s, mean_processing_time_finetuned_yolov8s_onnx],\n",
    "    \"Mean Inference Time (ms)\": [mean_inference_time_finetuned_yolov8n, mean_inference_time_finetuned_yolov8n_onnx], #mean_inference_time_finetuned_yolov8s, mean_inference_time_finetuned_yolov8s_onnx],\n",
    "    \"Mean Postprocessing Time (ms)\": [mean_postprocessing_time_finetuned_yolov8n, mean_postprocessing_time_finetuned_yolov8n_onnx], # mean_postprocessing_time_finetuned_yolov8s, mean_postprocessing_time_finetuned_yolov8s_onnx]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(speed_data_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real time object detection on a UAV video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# we create a video from frames since opencv videocapture method does not work with individual frames\n",
    "def create_video_from_frames(frame_folder, output_video_path, fps=30):\n",
    "    images = sorted([img for img in os.listdir(frame_folder) if img.endswith(\".jpg\")])\n",
    "    \n",
    "    if not images:\n",
    "        raise ValueError(f\"No JPG images found in {frame_folder}\")\n",
    "\n",
    "    first_frame = cv2.imread(os.path.join(frame_folder, images[0]))\n",
    "    if first_frame is None:\n",
    "        raise ValueError(f\"Unable to read the first frame: {images[0]}\")\n",
    "    \n",
    "    height, width, layers = first_frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for image in tqdm(images, desc=\"Creating video\"):\n",
    "        frame = cv2.imread(os.path.join(frame_folder, image))\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "        else:\n",
    "            print(f\"Warning: Unable to read frame {image}\")\n",
    "\n",
    "    video.release()\n",
    "    print(f\"Video saved to {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video_from_frames(\"../../data/videos/VisDrone2019-VID-test-dev/sequences/uav0000297_02761_v\", \"../../data/videos/VisDrone2019-VID-test-dev/sequences/uav0000297_02761_v/full_sequence_uav0000297_02761_v.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video_from_frames(\"../../data/videos/UAV-benchmark-S/S0102\", \"../../data/videos/UAV-benchmark-S/S0102/full_sequence_S0102.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video_from_frames(\"../../data/videos/VisDrone2019-VID-test-dev/sequences/uav0000201_00000_v\", \"../../data/videos/VisDrone2019-VID-test-dev/sequences/uav0000201_00000_v/full_sequence_uav0000201_00000_v.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video_from_frames(\"../../data/videos/VisDrone2019-VID-test-dev/sequences/uav0000306_00230_v\", \"../../data/videos/VisDrone2019-VID-test-dev/sequences/uav0000306_00230_v/full_sequence_uav0000306_00230_v.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def real_time_demo(video_path, model, window_name=\"Comparison\", model_name=None, save=None):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    running_mean_processing_time = 0\n",
    "    \n",
    "    # Getting video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if save:\n",
    "        output_path = f\"{save}/{model_name}_{video_path.split('/')[-1]}\"\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, video_fps, (width, height))\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Process frame\n",
    "        process_start = time.time()\n",
    "        results = model(frame, device='cpu', verbose=False, imgsz=640)\n",
    "        annotated_frame = results[0].plot()\n",
    "        process_end = time.time()\n",
    "        \n",
    "        process_time = process_end - process_start\n",
    "        \n",
    "        running_mean_processing_time = (running_mean_processing_time * (frame_count - 1) + process_time) / frame_count\n",
    "        \n",
    "        # Adding info to frame\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.7\n",
    "        color = (128, 0, 128) \n",
    "        thickness = 2\n",
    "        line_spacing = 30\n",
    "\n",
    "        cv2.putText(annotated_frame, f\"Model: {model_name}\", (10, 30), font, font_scale, color, thickness)\n",
    "        cv2.putText(annotated_frame, f\"Running Mean Processing Time: {running_mean_processing_time*1000:.2f} ms\", (10, 30 + line_spacing), font, font_scale, color, thickness)\n",
    "        cv2.putText(annotated_frame, f\"Video FPS: {video_fps:.2f}\", (10, 30 + 2*line_spacing), font, font_scale, color, thickness)\n",
    "        cv2.putText(annotated_frame, f\"Frame: {frame_count}/{total_frames}\", (10, 30 + 3*line_spacing), font, font_scale, color, thickness)\n",
    "        \n",
    "        cv2.imshow(window_name, annotated_frame)\n",
    "        \n",
    "        if save:\n",
    "            out.write(annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    if save:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return running_mean_processing_time*1000\n",
    "\n",
    "\n",
    "regular_yolov8n = YOLO(weights_path + \"/yolov8n.pt\")\n",
    "finetuned_yolov8n = YOLO(weights_path + \"/yolov8n_finetuned_visdrone_19_epochs.pt\")\n",
    "finetuned_onnx_yolov8n = YOLO(weights_path + \"/yolov8n_finetuned_visdrone_19_epochs.onnx\")\n",
    "\n",
    "# finetuned_yolov8s = YOLO(weights_path + \"/yolov8s_finetuned_visdrone_20_epochs.pt\")\n",
    "# finetuned_onnx_yolov8s = YOLO(weights_path + \"/yolov8s_finetuned_visdrone_20_epochs.onnx\")\n",
    "\n",
    "video_paths = [\n",
    "    \"../../data/videos/VisDrone2019-VID-test-dev/sequences/uav0000297_02761_v/full_sequence_uav0000297_02761_v.mp4\",\n",
    "    \"../../data/videos/UAV-benchmark-S/S0102/full_sequence_S0102.mp4\",\n",
    "    \"../../data/videos/VisDrone2019-VID-test-dev/sequences/uav0000201_00000_v/full_sequence_uav0000201_00000_v.mp4\",\n",
    "    \"../../data/videos/VisDrone2019-VID-test-dev/sequences/uav0000306_00230_v/full_sequence_uav0000306_00230_v.mp4\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../models/yolov8n_finetuned_visdrone_19_epochs.onnx for ONNX Runtime inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/rzhqtj2d2bl2tcv7_p857byr0000gn/T/ipykernel_70047/3129104125.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  speed_df = pd.concat([speed_df, pd.DataFrame({\"Model\": [model_name], \"Processing Time (ms)\": [avg_processing_time]})])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Processing Time (ms)\n",
      "0       Finetuned YOLOv8n             44.314221\n",
      "0  Finetuned YOLOv8n ONNX             48.046318\n"
     ]
    }
   ],
   "source": [
    "# Running comparisons between the models\n",
    "import pandas as pd\n",
    "\n",
    "results_path_real_time = \"../../results/real_time_demo\"\n",
    "if not os.path.exists(results_path_real_time):\n",
    "    os.makedirs(results_path_real_time)\n",
    "    \n",
    "speed_df = pd.DataFrame(columns=[\"Model\", \"Processing Time (ms)\"])\n",
    "models = {\"Regular YOLOv8n\": regular_yolov8n, \"Finetuned YOLOv8n\": finetuned_yolov8n, \"Finetuned YOLOv8n ONNX\": finetuned_onnx_yolov8n} #\"Finetuned YOLOv8s\": finetuned_yolov8s, \"Finetuned YOLOv8s ONNX\": finetuned_onnx_yolov8s}\n",
    "results_paths_videos = {\"Regular YOLOv8n\": os.path.join(results_path_real_time, \"regular_yolov8n\"), \"Finetuned YOLOv8n\": os.path.join(results_path_real_time, \"finetuned_yolov8n\"), \"Finetuned YOLOv8n ONNX\": os.path.join(results_path_real_time, \"finetuned_yolov8n_onnx\")} #\"Finetuned YOLOv8s\": os.path.join(results_path, \"finetuned_yolov8s\"), \"Finetuned YOLOv8s ONNX\": os.path.join(results_path, \"finetuned_yolov8s_onnx\")}\n",
    "for model_name, model in models.items():\n",
    "    if not os.path.exists(results_paths_videos[model_name]):\n",
    "        os.makedirs(results_paths_videos[model_name])\n",
    "    \n",
    "for model_name, model in models.items():\n",
    "    avg_processing_time = 0\n",
    "    for video_path in video_paths:\n",
    "        mean_processing_time  = real_time_demo(video_path, model, window_name=f\"{model_name}\", model_name=model_name, save=results_paths_videos[model_name])\n",
    "        avg_processing_time += mean_processing_time\n",
    "    avg_processing_time /= len(video_paths)\n",
    "    if model_name == \"Regular YOLOv8n\":\n",
    "        # continuing the loop without adding the processing time of the regular model, since the comparison we are interested in is with the finetuned model\n",
    "        continue\n",
    "    else: \n",
    "        speed_df = pd.concat([speed_df, pd.DataFrame({\"Model\": [model_name], \"Processing Time (ms)\": [avg_processing_time]})])\n",
    "    \n",
    "print(speed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracking\n",
    "# using the tracking feature of yolov8 in ultralytics, that allows to persist tracks between frames\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2 \n",
    "\n",
    "fine_tuned_yolov8n = YOLO(weights_path + \"yolov8n_finetuned_visdrone_19_epochs.pt\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"../../data/videos/VisDrone2019-VID-test-dev/sequences/uav0000201_00000_v/full_sequence_uav0000201_00000_v.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = fine_tuned_yolov8n.track(frame, persist=True, device=\"cpu\", verbose=False, imgsz=640)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preligens_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
