# Answers to Theoretical Questions of Part I (Part I System design and architecture)

Below are all the answers to the theoretical questions of the technical interview.


1. Discuss a few strategies for implementing a embeded AI for objectf detection on full motion video from UAV: 

Object detection on full-motion video is a crucial application for agriculture to monitor crops, for surveillance in the identification of key targets on the field, and in other countless domains. Running it on a UAV (unmanned aerial vehicle) such as a drone involves having to run models and algorithms on relatively small configurations with limited compute and memory. It is for that reason that we would have to find a tradeoff between performance and efficiency, by picking an optimized model like TinyYOLO and MobileNet, both optimized to run on devices with limited resources. These models have been tested on embedded devices and provide a good balance between performance and resource usage. A UAV would also require real-time processing and near-instantaneous response, as cameras in full-motion video capture 30 frames per second or more, most of the time. Heavier models that are unoptimized for embedded devices could exhibit significant processing latency, leading to unresponsive object detection. That corroborates the need for using TinyYOLO and MobileNet which are well-suited for this use.

When it comes to the implementation of the model, the task at hand requires training the chosen model on a dataset, which could be a publicly available dataset like UAVDT (UAV Detection and Tracking) that provides 80,000 annotated images captured by UAVs in various scenarios. This dataset offers a solid initial set of data to run tests. At a later stage, we could fine-tune the obtained model on a specific in-house dataset to improve its accuracy. I would use the PyTorch framework to train the model, as it is the framework I am most familiar with and is a reference framework for model training in the industry.

The implementation would then require us to create an inference pipeline. I would convert the trained model to the ONNX format, which traces the graph of the network and provides a universal format compatible with other languages besides Python. The inference pipeline would be created in C++, capturing frames continuously and performing inference by loading the model and annotating each frame with the bounding boxes of objects of interest, then overlaying the bounding boxes of detected objects on top of each input frame. I would also use TensorRT to optimize the inference and deploy the model effectively.

Once this is done, I would conduct a series of tests with this configuration to assess whether the latency and accuracy are acceptable using test images and out-of-distribution images that the model hasn’t seen yet. Subsequently, I would implement optimizations such as quantization, which involves converting the model weights from floating points to integers, thereby reducing memory usage and improving inference speed. Additionally, I could perform pruning to further optimize the inference pipeline. Ultimately, the goal is to obtain a routine that captures images, preprocesses them, runs the model, and detects objects in real-time.




2. What are the challenges for optimizing such a system on an embedded platform? Do you recommend specific stacks, hardware or technology and why?

Optimizing a system on an embedded platform involves several challenges related to both the software and hardware stacks, as well as the specific technologies and libraries used.

When we talk about stacks in this context, we refer to both the software and hardware components that constitute the system. On the software side, I am including the operating system, libraries, and frameworks used to develop and run the application. On the hardware side, I include the physical components like the processor, memory, and other peripherals. Optimizing the software stack means choosing lightweight and efficient components that are compatible with the hardware. I would probably use a streamlined Linux distribution tailored for embedded systems, which could reduce overhead and improve performance. Additionally, selecting optimized libraries such as OpenCV for image processing and PyTorch for machine learning ensures that the software can effectively utilize the hardware capabilities.

The choice of hardware is crucial for achieving optimal performance on an embedded platform. I chose the Jetson Xavier NX because it offers a powerful GPU architecture designed specifically for AI and computer vision tasks, along with ample RAM and energy efficiency, making it ideal for real-time processing on UAVs. The Jetson Xavier NX’s support for TensorRT provides significant advantages in terms of model optimization and inference speed. Compared to alternatives like the Raspberry Pi or Google Coral, the Jetson Xavier NX delivers superior performance for the compute-intensive tasks required in this project. While other Jetson models such as the Jetson Nano or Jetson Orin could be considered, the Xavier NX offers a good balance of power and efficiency for this specific use case to the best of my knowledge after some research.

In addition to computational power and efficiency, several other factors must be considered. The weight of the hardware is crucial, especially for UAV applications where payload capacity is limited. The Jetson Xavier NX, while powerful, is relatively lightweight compared to more powerful options like the Orin, making it a suitable choice for UAVs where weight is a concern. Furthermore, the I/O capabilities are significant for interfacing with cameras, sensors, and other peripherals. The Jetson Xavier NX provides extensive I/O options, which is advantageous for complex applications but may be overkill for simpler tasks.

Price is another critical factor. The Jetson Xavier NX offers high performance at a reasonable cost, making it a more affordable option compared to the Jetson Orin. For budget-sensitive projects, this balance between cost and performance makes the Xavier NX an attractive choice.

Form factor and physical size also play a role. The compactness of the hardware can affect the design and aerodynamics of the UAV. The Jetson Xavier NX has a smaller form factor than the Orin, offering advantages in this regard while still providing substantial computational power.

The technologies we would choose are also key to optimizing the system. Essential technologies include machine learning frameworks, optimization tools, and inference engines. I would select PyTorch for model training due to its flexibility and extensive support for various neural network architectures. Once the model is trained, exporting it to the ONNX format would allow for interoperability and further optimization. TensorRT could then be used to optimize the model for NVIDIA hardware, significantly enhancing inference speed and efficiency. These technologies collectively support the goal of real-time processing and efficient resource usage on the embedded platform.

The workflow involving PyTorch for training, exporting to ONNX, and utilizing TensorRT for inference is chosen for several reasons that I mentioned before. PyTorch offers ease of use and flexibility during the development and training phase. Exporting the model to ONNX provides compatibility with various platforms and optimization tools. TensorRT is specifically designed to optimize models for NVIDIA GPUs, offering significant performance improvements in terms of inference speed and efficiency. This workflow ensures that the system can meet the real-time processing requirements while operating within the constraints of an embedded platform.

3. What factors could impact the quality of the algorithm? How would you mitigate such risk?

Several factors can impact the quality of the algorithm used for object detection on a UAV, and understanding these factors is crucial to mitigate potential risks.

Firstly, the quality and quantity of data used to train the algorithm are paramount. Poor quality data or an insufficient amount can lead to a model that performs poorly. To mitigate this, it would be essential that I employ data augmentation techniques such as rotation, flipping, and scaling to increase the dataset’s size and diversity. Additionally, data cleaning processes should be set up place to ensure the data is free from errors and inconsistencies. I would also ensure that I am working with a balanced dataset, as it is also critical to avoid bias towards certain classes, which can skew the model’s performance.

Overfitting and underfitting are common challenges that can significantly impact an algorithm’s effectiveness. Overfitting occurs when the model learns the training data too well and fails to generalize to new data, while underfitting happens when the model is too simple to capture the data’s underlying patterns. To prevent these issues, I would employ techniques like L1/L2 regularization to prevent overfitting. Using cross-validation would also help ensure the model generalizes well to unseen data, and careful model selection would be necessary to choose a model appropriately complex for the task at hand.

The complexity of the algorithm itself is another factor. While more complex algorithms might perform better, they can also be more prone to overfitting and require more computational resources. Once the training is done, using pruning and quantization could simplify the model, reducing its complexity without significantly impacting performance. Hyper-parameter tuning is also essential to find the best balance between complexity and performance.

Model drift is a concern over time, as the performance of the model can degrade if the data it encounters in production changes significantly from the training data. Continuous monitoring of the model’s performance in production would be crucial, with retraining as necessary to maintain its effectiveness. Implementing incremental learning methods allows the model to learn from new data without needing to retrain from scratch.

Even the best algorithm can perform poorly if the hardware cannot support it adequately. Using hardware accelerators like GPUs or TPUs would help meet the algorithm’s computational needs. Ensuring the algorithm is implemented efficiently is also vital to make the best use of the available hardware resources.

Finally, ensuring the algorithm meets real-time processing requirements is critical for applications like UAVs, where rapid response times are necessary. Using optimization tools like TensorRT can optimize the model for faster inference. Continuously measuring and optimizing latency helps ensure real-time performance requirements are met.

By addressing these factors and implementing appropriate mitigation strategies, the quality of the algorithm would be maintained, ensuring reliable and effective performance in real-world applications.